{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIMvgrGMe7ZF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n25wrPRbfCGc"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyGUj_q7IdfQ"
      },
      "source": [
        "# Dataset Collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpO0um1nez_q"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/datasets/determinism\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AnxnW65I_FC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/sh: line 1: pip: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tfds-nightly tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hxMPT0wIu3f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at0bMS_jIdjt"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Dataset collections provide a simple way to group together an arbitrary number\n",
        "of existing TFDS datasets, and to perform simple operations over them.\n",
        "\n",
        "They can be useful, for example, to group together different datasets related to the same task, or for easy [benchmarking](https://ruder.io/nlp-benchmarking/) of models over a fixed number of different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLvkZBKwIdmL"
      },
      "source": [
        "## Find available dataset collections\n",
        "\n",
        "All dataset collection builders are subclass of\n",
        "`tfds.core.dataset_collection_builder.DatasetCollection`.\n",
        "\n",
        "To get the list of available builders, use `tfds.list_dataset_collections()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R14uGGzKItDz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['longt5', 'xtreme']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfds.list_dataset_collections()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpcq2AMvI5K1"
      },
      "source": [
        "## Load and inspect a dataset collection\n",
        "\n",
        "The easiest way of loading a dataset collection is to instantiate a `DatasetCollectionLoader` object using the [`tfds.dataset_collection`](https://www.tensorflow.org/datasets/api_docs/python/tfds/dataset_collection) command.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leIwyl9aI3WA"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('longt5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgjomybjY7qI"
      },
      "source": [
        "Specific dataset collection versions can be loaded following the same syntax as with TFDS datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyILkuYJY6ts"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('longt5:1.0.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKOJ6CNQKG9S"
      },
      "source": [
        "A dataset collection loader can display information about the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwk4PVDoKEAC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset collection: longt5\n",
            "Version: 1.0.0\n",
            "Description: # Long T5 benchmark\n",
            "\n",
            "This dataset collection comprises the evaluation benchmark used in the paper:\n",
            "_LongT5: Efficient Text-To-Text Transformer for Long Sequences_\n",
            "\n",
            "LongT5 is an extension of the T5 model that handles long sequence inputs more\n",
            "efficiently. LongT5 achieves state-of-the-art performance on several\n",
            "summarization benchmarks that required longer context or multi-document\n",
            "understanding.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "collection_loader.print_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FlLLbwuLLTu"
      },
      "source": [
        "And it can also display information about the datasets contained in the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxNJEie6K55T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset collection longt5 (version: 1.0.0) contains the datasets:\n",
            " - natural_questions: DatasetReference(dataset_name='natural_questions', version=Version('0.1.0'), split_mapping=None, config='longt5')\n",
            " - media_sum: DatasetReference(dataset_name='media_sum', version=Version('1.0.0'), split_mapping=None, config=None)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "collection_loader.print_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxorc3kLwRj"
      },
      "source": [
        "### Loading datasets from a dataset collection\n",
        "\n",
        "The easiest way to load one dataset from a collection is to use `DatasetCollectionLoader`'s `load_dataset` method, which loads the required dataset by calling [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
        "\n",
        "It will return a dictionary of split names and the corresponding `tf.data.Dataset`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1nRj4ILwb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e,\n",
              " 'validation': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection_loader.load_dataset(\"natural_questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2spLEgN1Lwmm"
      },
      "source": [
        "`load_dataset` accepts the following optional parameters:\n",
        "\n",
        "* `split`: which split(s) to load. It accepts a single split (`split=\"test\"`) or a list of splits: (`split=[\"train\", \"test\"]`). If not specified, it will load all splits for the given dataset.\n",
        "* `loader_kwargs`: keyword arguments to be passed to the `tfds.load` function. Refer to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) documentation for a comperehensive overview of the different loading options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aClLU4eAh2oC"
      },
      "source": [
        "### Loading multiple datasets from a dataset collection\n",
        "\n",
        "The easiest way to load multiple datasets from a collection is to use `DatasetCollectionLoader`'s `load_datasets` method, which loads the required dataset by calling [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
        "\n",
        "It will return a dictionary of dataset names, each one of which is associated with a dictionary of split names and the corresponding `tf.data.Dataset`s, as in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEG5744Oh2vQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'media_sum': {'test': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e,\n",
              "  'train': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e,\n",
              "  'val': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e},\n",
              " 'natural_questions': {'train': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e,\n",
              "  'validation': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection_loader.load_datasets(['natural_questions', 'media_sum'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0kNqwsiN1Y"
      },
      "source": [
        "`load_all_datasets` will load *all* available datasets for a given collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX-M3xcjiM35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'media_sum': {'test': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e,\n",
              "  'train': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e,\n",
              "  'val': \u003cPrefetchDataset element_spec={'date': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'program': TensorSpec(shape=(), dtype=tf.string, name=None), 'speaker': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'summary': TensorSpec(shape=(), dtype=tf.string, name=None), 'url': TensorSpec(shape=(), dtype=tf.string, name=None), 'utt': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e},\n",
              " 'natural_questions': {'train': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e,\n",
              "  'validation': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(), dtype=tf.string, name=None), 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}\u003e}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection_loader.load_all_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXxVztK5kAHh"
      },
      "source": [
        "`load_datasets` accepts the following optional parameters:\n",
        "\n",
        "* `split`: which split(s) to load. It accepts a single split `(split=\"test\")` or a list of splits: `(split=[\"train\", \"test\"])`. If not specified, it will load all splits for the given dataset.\n",
        "* `loader_kwargs`: keyword arguments to be passed to the `tfds.load` function. Refer to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) documentation for a comperehensive overview of the different loading options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JoreSHfcKZ"
      },
      "source": [
        "### Specifying `loader_kwargs`\n",
        "\n",
        "`loader_kwargs` are optional keyword arguments to be passed to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) function.\n",
        "They can be specified in three ways:\n",
        "\n",
        "1. When initializing the `DatasetCollectionLoader` class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjgZSIlvfcSP"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('longt5', loader_kwargs=dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8SIsOLqk1Yg"
      },
      "source": [
        "2. Using `DatasetCollectioLoader`'s `set_loader_kwargs` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrysflp-k1d3"
      },
      "outputs": [],
      "source": [
        "collection_loader.set_loader_kwargs(dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbiY8CtGk1jK"
      },
      "source": [
        "3. As optional paramenters to the `load_dataset`, `load_datasets` and `load_all_datasets` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHSu-8GnlGTk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': \u003cPrefetchDataset element_spec={'all_answers': TensorSpec(shape=(None, None), dtype=tf.string, name=None), 'answer': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'context': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'question': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'title': TensorSpec(shape=(None,), dtype=tf.string, name=None)}\u003e}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection_loader.load_dataset('natural_questions', loader_kwargs=dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btd8pLIslGa_"
      },
      "source": [
        "## Add a new dataset collection\n",
        "\n",
        "All dataset collections are implemented subclasses of `tfds.core.dataset_collection_builder.DatasetCollection`.\n",
        "\n",
        "Here is a minimal example of a dataset collection builder, defined in the file `my_dataset_collection.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLBn6SqwmIVr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from typing import Mapping\n",
        "from tensorflow_datasets.core import dataset_collection_builder\n",
        "\n",
        "class MyDatasetCollection(dataset_collection_builder.DatasetCollection):\n",
        "  \"\"\"Dataset collection builder my_dataset_collection.\"\"\"\n",
        "\n",
        "  @property\n",
        "  def info(self) -\u003e dataset_collection_builder.DatasetCollectionInfo:\n",
        "    return dataset_collection_builder.DatasetCollectionInfo.from_cls(\n",
        "        dataset_collection_class=self.__class__,\n",
        "        description=\"my_dataset_collection description.\",\n",
        "        release_notes={\n",
        "            \"1.0.0\": \"Initial release\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def datasets(\n",
        "      self,\n",
        "  ) -\u003e Mapping[str, Mapping[str, dataset_collection_builder.DatasetReference]]:\n",
        "    return collections.OrderedDict({\n",
        "        \"1.0.0\":\n",
        "            dataset_collection_builder.references_for({\n",
        "                \"dataset_1\": \"natural_questions/default:0.0.2\",\n",
        "                \"dataset_2\": \"media_sum:1.0.0\",\n",
        "            }),\n",
        "        \"1.1.0\":\n",
        "            dataset_collection_builder.references_for({\n",
        "                \"dataset_1\": \"natural_questions/longt5:0.1.0\",\n",
        "                \"dataset_2\": \"media_sum:1.0.0\",\n",
        "                \"dataset_3\": \"squad:3.0.0\"\n",
        "            })\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmEGDm5NqSr8"
      },
      "source": [
        "Let's see in detail the 2 abstract methods to overwrite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxwSa-TzmIa1"
      },
      "source": [
        "### `info`: dataset collection metadata\n",
        "\n",
        "`info` returns the `dataset_collection_builder.DatasetCollectionInfo` containing the collection's metadata.\n",
        "\n",
        "The dataset collection info contains four fields:\n",
        "\n",
        "- `name`: the name of the dataset collection.\n",
        "- `description`: a markdown-formatted description of the dataset collection. There are two ways to define a dataset collection's description:\n",
        "  - As a (multi-line) string directly in the colletion's `my_dataset_collection.py` file - similarly as it is already done for TFDS datasets;\n",
        "  - In a `description.md` file, which must be placed in the dataset collection folder.\n",
        "- `release_notes`: a mapping from the dataset collection's version to the corresponding release notes.\n",
        "- `citation`: An optional (list of) `BibTeX` citation(s) for the dataset collection. There are two ways to define a dataset collection's citation:\n",
        "  - As a (multi-line) string directly in the colletion's `my_dataset_collection.py` file - similarly as it is already done for TFDS datasets;\n",
        "  - In a `citations.bib` file, which must be placed in the dataset collection folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSXh_C2tmIju"
      },
      "source": [
        "### `dataset`: define the datasets in the collection\n",
        "\n",
        "`dataset` returns the TFDS datasets included as part of the collection.\n",
        "\n",
        "`dataset` is defined as a dictionary of versions, which describe the evolution of the dataset collection.\n",
        "\n",
        "For each version, the included TFDS datasets are stored as a dictionary from dataset names to `dataset_collection_builder.DatasetReference`. For example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5FzSvly4Gcb"
      },
      "outputs": [],
      "source": [
        "  @property\n",
        "  def datasets(self):\n",
        "    return {\n",
        "        \"1.0.0\": {\n",
        "            \"yes_no\":\n",
        "                dataset_collection_builder.DatasetReference(\n",
        "                    dataset_name=\"yes_no\", version=\"1.0.0\"),\n",
        "            \"sst2\":\n",
        "                dataset_collection_builder.DatasetReference(\n",
        "                    dataset_name=\"glue\", config=\"sst2\", version=\"2.0.0\"),\n",
        "            \"assin2\":\n",
        "                dataset_collection_builder.DatasetReference(\n",
        "                    dataset_name=\"assin2\", version=\"1.0.0\"),\n",
        "        },\n",
        "        # ...\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGg8ojX1ai_"
      },
      "source": [
        "The `dataset_collection_builder.references_for` method provides a more compact way to express the same as above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVp8APob4T-y"
      },
      "outputs": [],
      "source": [
        "@property\n",
        "def datasets(self):\n",
        "  return {\n",
        "      \"1.0.0\":\n",
        "          dataset_collection_builder.references_for({\n",
        "              \"yes_no\": \"yes_no:1.0.0\",\n",
        "              \"sst2\": \"glue/sst:2.0.0\",\n",
        "              \"assin2\": \"assin2:1.0.0\",\n",
        "          }),\n",
        "      # ...\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJDGoeAqmJAQ"
      },
      "source": [
        "### Send us feedback\n",
        "\n",
        "We are continuously trying to improve the dataset creation workflow, but can\n",
        "only do so if we are aware of the issues. Which issues, errors did you\n",
        "encountered while creating the dataset collection? Was there a part which was confusing,\n",
        "boilerplate or wasn't working the first time? Please share your feedback on\n",
        "[GitHub](https://github.com/tensorflow/datasets/issues)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dataset_collections.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
